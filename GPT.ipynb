{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "LM Huggingface finetune GPT-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.13 64-bit ('GPUEnv': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.7.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "67a92ed351ae01a7fd8b033e7a22a2c19e5485e3a18cc30ae9cf92342c2b765b"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Finetuning GPT2 using HuggingFace and Tensorflow**\n",
        "\n",
        "In this colab notebook we set up a simple outline of how you can use Huggingface to fine tune a gpt2 model on finance titles to generate new possible headlines. This notebook uses the hugginface finefuning scripts and then uses the TensorFlow version of the genreated models."
      ],
      "metadata": {
        "id": "MZMbGCw0Qxfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First begin setup by cloning transformers repo. We need to store the training script locally since there isnt an easier way to train tf based gpt2 models as far as I can see."
      ],
      "metadata": {
        "id": "PzB7m0GI5fSt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "#Clone the transformers repo into the notebook\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 108786, done.\u001b[K\n",
            "remote: Total 108786 (delta 0), reused 0 (delta 0), pack-reused 108786\u001b[K\n",
            "Receiving objects: 100% (108786/108786), 95.51 MiB | 23.89 MiB/s, done.\n",
            "Resolving deltas: 100% (79274/79274), done.\n"
          ]
        }
      ],
      "metadata": {
        "id": "91rmSAUQVIUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e10b71-6fa8-4e32-8b09-1d6c53d27680"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "! pip install tensorflow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (58.0.4)\n",
            "Requirement already satisfied: gast>=0.2.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (3.20.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorflow) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "%cd .."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/komleva/gen_money\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "!pip install -e ."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///home/komleva/gen_money/transformers\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (1.21.5)\n",
            "Requirement already satisfied: requests in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: sacremoses in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (0.0.49)\n",
            "Requirement already satisfied: importlib-metadata in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: filelock in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (2022.3.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests->transformers==4.19.0.dev0) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests->transformers==4.19.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests->transformers==4.19.0.dev0) (1.26.9)\n",
            "Requirement already satisfied: six in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from sacremoses->transformers==4.19.0.dev0) (1.16.0)\n",
            "Requirement already satisfied: click in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from sacremoses->transformers==4.19.0.dev0) (8.1.2)\n",
            "Requirement already satisfied: joblib in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.19.0.dev0\n",
            "    Uninstalling transformers-4.19.0.dev0:\n",
            "      Successfully uninstalled transformers-4.19.0.dev0\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed transformers-4.19.0.dev0\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Clone should now be in the machine\n",
        "!ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cur.csv\n",
            "cur.txt\n",
            "envGPT\n",
            "eval_tmp.txt\n",
            "gpt_gen.ipynb\n",
            "GPT.ipynb\n",
            "gpt.py\n",
            "LM_Huggingface_finetune_GPT_2.ipynb\n",
            "out\n",
            "test.txt\n",
            "train_tmp.txt\n",
            "train.txt\n",
            "transformers\n",
            "unreg.txt\n",
            "Копия_блокнота__Komleva_Text_Generation_ipynb_.ipynb\n"
          ]
        }
      ],
      "metadata": {
        "id": "RFzEr4Y1VJKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b9a4c6-e068-46bd-d600-37c365ded8f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check to see what gpu we were granted. For Colab Pro it will vary between a Tesla V100 or P100. For normal colab it should be a k80"
      ],
      "metadata": {
        "id": "M-ke4920tDqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "!nvidia-smi"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr  7 08:59:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Quadro RTX 5000     On   | 00000000:D8:00.0  On |                  Off |\n",
            "| 33%   36C    P2    46W / 230W |   1758MiB / 16384MiB |      5%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A     34424      G   /usr/lib/xorg/Xorg                135MiB |\n",
            "|    0   N/A  N/A     34909      G   /usr/bin/plasmashell               40MiB |\n",
            "|    0   N/A  N/A     35407      G   ...akonadi_archivemail_agent        2MiB |\n",
            "|    0   N/A  N/A     35422      G   .../akonadi_mailfilter_agent        2MiB |\n",
            "|    0   N/A  N/A     35426      G   ...n/akonadi_sendlater_agent       26MiB |\n",
            "|    0   N/A  N/A     35427      G   ...nadi_unifiedmailbox_agent        2MiB |\n",
            "|    0   N/A  N/A     47021      G   ...bexec/kscreenlocker_greet       78MiB |\n",
            "|    0   N/A  N/A     56727      G   /usr/lib/xorg/Xorg                 79MiB |\n",
            "|    0   N/A  N/A     57464      G   /usr/bin/plasmashell               63MiB |\n",
            "|    0   N/A  N/A     63729      G   ...bexec/kscreenlocker_greet      102MiB |\n",
            "|    0   N/A  N/A     94775      C   python                           1213MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "metadata": {
        "id": "BPIgByLqmI81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68ded87-37bb-4156-d5f4-0df01133f3e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change directory location to be in the examples folder and then install any requirements"
      ],
      "metadata": {
        "id": "XOTdb4rWv8YN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os\n",
        "os.chdir(\"transformers\")\n",
        "os.chdir(\"./examples/tensorflow/language-modeling\")\n",
        "!ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md  requirements.txt  run_clm.py  run_mlm.py\n"
          ]
        }
      ],
      "metadata": {
        "id": "K2M2Oz9CYB4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fa2b00-20c9-497c-8ba6-9ed85843650a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "!pip install -r requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets>=1.8.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.96)\n",
            "Requirement already satisfied: responses<0.19 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (7.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.64.0)\n",
            "Requirement already satisfied: dill in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: packaging in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: multiprocess in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.70.12.2)\n",
            "Requirement already satisfied: aiohttp in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (2022.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (4.11.3)\n",
            "Requirement already satisfied: pandas in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from datasets>=1.8.0->-r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: pyyaml in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: filelock in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 1)) (3.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 1)) (1.26.9)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 1)) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.8.0->-r requirements.txt (line 1)) (1.16.0)\n"
          ]
        }
      ],
      "metadata": {
        "id": "04rBGxwiYnep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3902c408-0ab6-49b9-8ed5-5d7f5bd6cb54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "!ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md  requirements.txt  run_clm.py  run_mlm.py\n"
          ]
        }
      ],
      "metadata": {
        "id": "iB29mAKjaNIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf082656-d9de-44b4-c812-44757a196c32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "!pip install pyarrow --upgrade"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pyarrow) (1.21.5)\n"
          ]
        }
      ],
      "metadata": {
        "id": "eo5gRmXaWx0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4413970a-159b-484f-b090-106f50be1434"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\n",
        "os.chdir(\"/home/komleva/gen_money/transformers\")\n",
        "os.chdir(\"./examples/pytorch/language-modeling\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "y5sdYSpAWY1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up data from a text file in the format <|title|> some data <|endoftext|> and split into training and eval sets."
      ],
      "metadata": {
        "id": "oDuJnVmrY_Fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "\"\"\"\n",
        "Now load the data line by line\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('/home/komleva/gen_money/data/train3.txt', 'r') as data:\n",
        "  dataset = [\"<|title|>\" + x.strip() for x in data.readlines()]\n",
        "\n",
        "train, eval = train_test_split(dataset, train_size=.9, random_state=2020)\n",
        "print(\"training size:\" + str(len(train)))\n",
        "print(\"Evaluation size: \" + str(len(eval)))\n",
        "\n",
        "with open('/home/komleva/gen_money/data/train_tmp.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(train))\n",
        "\n",
        "with open('/home/komleva/gen_money/data/eval_tmp.txt', 'w') as file_handle:\n",
        "  file_handle.write(\"<|endoftext|>\".join(eval))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training size:8565\n",
            "Evaluation size: 952\n"
          ]
        }
      ],
      "metadata": {
        "id": "uU4ckPTf9T-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bcdab3a-f95d-4e99-b7fc-993422fad68f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script below will fine tune GPT2 on your text data that you setup above. This training step will take anywhre from tens of minutes to hours depending on how large your training set is, how many epochs you intend to train on, and if you are using colab or colab pro. We utilize mixed precision in this model to shave off some training time. For a large data set I was using for another experiment it saved us over 30 mins in training time."
      ],
      "metadata": {
        "id": "1TYi8Oqs6Npo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "!python run_clm.py \\\n",
        "--model_type gpt2-medium \\\n",
        "--model_name_or_path gpt2-medium \\\n",
        "--train_file \"/home/komleva/gen_money/data/train_tmp.txt\" \\\n",
        "--do_train \\\n",
        "--validation_file \"/home/komleva/gen_money/data/eval_tmp.txt\" \\\n",
        "--do_eval \\\n",
        "--per_gpu_train_batch_size 1 \\\n",
        "--save_steps -1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--fp16 \\\n",
        "--output_dir=\"/home/komleva/gen_money/out\"\\\n",
        "--overwrite_output_dir"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/16/2022 12:50:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "05/16/2022 12:50:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/home/komleva/gen_money/out/runs/May16_12-50-35_admin,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/home/komleva/gen_money/out,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/home/komleva/gen_money/out,\n",
            "save_on_each_node=False,\n",
            "save_steps=-1,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/16/2022 12:50:36 - WARNING - datasets.builder - Using custom data configuration default-689a4337711f8457\n",
            "05/16/2022 12:50:36 - INFO - datasets.builder - Generating dataset text (/home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "Downloading and preparing dataset text/default to /home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n",
            "Downloading data files: 100%|███████████████████| 2/2 [00:00<00:00, 5405.03it/s]\n",
            "05/16/2022 12:50:36 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "05/16/2022 12:50:36 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 2007.80it/s]\n",
            "05/16/2022 12:50:36 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "05/16/2022 12:50:36 - INFO - datasets.builder - Generating train split\n",
            "05/16/2022 12:50:36 - INFO - datasets.builder - Generating validation split\n",
            "05/16/2022 12:50:36 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n",
            "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1285.02it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-05-16 12:50:36,812 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/komleva/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:690] 2022-05-16 12:50:36,814 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2022-05-16 12:50:37,325 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2022-05-16 12:50:37,823 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/komleva/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:690] 2022-05-16 12:50:37,825 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-16 12:50:41,512 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /home/komleva/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-16 12:50:41,512 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /home/komleva/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-16 12:50:41,512 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /home/komleva/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-16 12:50:41,512 >> loading file https://huggingface.co/gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-16 12:50:41,513 >> loading file https://huggingface.co/gpt2-medium/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-05-16 12:50:41,513 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2022-05-16 12:50:42,013 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/komleva/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:690] 2022-05-16 12:50:42,015 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-05-16 12:50:42,722 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /home/komleva/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:2057] 2022-05-16 12:50:46,324 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2066] 2022-05-16 12:50:46,324 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3390] 2022-05-16 12:50:50,547 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1543084 > 1024). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|run_clm.py:393] 2022-05-16 12:50:50,548 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
            "05/16/2022 12:50:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-575ab1c81d2ec7ef.arrow\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:04<00:00,  4.53s/ba]\n",
            "Running tokenizer on dataset:   0%|                       | 0/1 [00:00<?, ?ba/s]05/16/2022 12:50:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-10d871e09fb25c14.arrow\n",
            "Running tokenizer on dataset: 100%|███████████████| 1/1 [00:00<00:00,  2.08ba/s]\n",
            "Grouping texts in chunks of 1024:   0%|                   | 0/1 [00:00<?, ?ba/s]05/16/2022 12:50:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-eac08c773a08fb27.arrow\n",
            "Grouping texts in chunks of 1024: 100%|███████████| 1/1 [00:01<00:00,  1.22s/ba]\n",
            "Grouping texts in chunks of 1024:   0%|                   | 0/1 [00:00<?, ?ba/s]05/16/2022 12:50:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/komleva/.cache/huggingface/datasets/text/default-689a4337711f8457/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-6f12c214c5801bda.arrow\n",
            "Grouping texts in chunks of 1024: 100%|███████████| 1/1 [00:00<00:00,  7.50ba/s]\n",
            "[INFO|trainer.py:453] 2022-05-16 12:50:56,056 >> Using amp half precision backend\n",
            "[WARNING|training_args.py:999] 2022-05-16 12:50:56,058 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:999] 2022-05-16 12:50:56,058 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/home/komleva/gen_money/transformers/src/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-05-16 12:50:56,063 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-05-16 12:50:56,063 >>   Num examples = 1506\n",
            "[INFO|trainer.py:1292] 2022-05-16 12:50:56,063 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1293] 2022-05-16 12:50:56,063 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1294] 2022-05-16 12:50:56,063 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1295] 2022-05-16 12:50:56,063 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-05-16 12:50:56,063 >>   Total optimization steps = 7530\n",
            "[WARNING|training_args.py:999] 2022-05-16 12:50:56,067 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 2.4442, 'learning_rate': 4.670650730411687e-05, 'epoch': 0.33}\n",
            "{'loss': 2.023, 'learning_rate': 4.338645418326693e-05, 'epoch': 0.66}\n",
            "{'loss': 1.8777, 'learning_rate': 4.0066401062417e-05, 'epoch': 1.0}\n",
            "{'loss': 1.5222, 'learning_rate': 3.674634794156707e-05, 'epoch': 1.33}\n",
            "{'loss': 1.4604, 'learning_rate': 3.342629482071714e-05, 'epoch': 1.66}\n",
            "{'loss': 1.3875, 'learning_rate': 3.01062416998672e-05, 'epoch': 1.99}\n",
            "{'loss': 1.1855, 'learning_rate': 2.6786188579017264e-05, 'epoch': 2.32}\n",
            "{'loss': 1.1692, 'learning_rate': 2.3466135458167332e-05, 'epoch': 2.66}\n",
            "{'loss': 1.1176, 'learning_rate': 2.0146082337317398e-05, 'epoch': 2.99}\n",
            "{'loss': 1.0182, 'learning_rate': 1.6826029216467466e-05, 'epoch': 3.32}\n",
            "{'loss': 0.9645, 'learning_rate': 1.3505976095617531e-05, 'epoch': 3.65}\n",
            "{'loss': 0.9855, 'learning_rate': 1.0185922974767596e-05, 'epoch': 3.98}\n",
            "{'loss': 0.8748, 'learning_rate': 6.865869853917662e-06, 'epoch': 4.32}\n",
            "{'loss': 0.8849, 'learning_rate': 3.5524568393094293e-06, 'epoch': 4.65}\n",
            "{'loss': 0.8979, 'learning_rate': 2.3240371845949537e-07, 'epoch': 4.98}\n",
            "100%|███████████████████████████████████████| 7530/7530 [47:32<00:00,  2.64it/s][INFO|trainer.py:1530] 2022-05-16 13:38:28,283 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2852.2206, 'train_samples_per_second': 2.64, 'train_steps_per_second': 2.64, 'train_loss': 1.3190794999222673, 'epoch': 5.0}\n",
            "100%|███████████████████████████████████████| 7530/7530 [47:32<00:00,  2.64it/s]\n",
            "[INFO|trainer.py:2166] 2022-05-16 13:38:28,286 >> Saving model checkpoint to /home/komleva/gen_money/out\n",
            "[INFO|configuration_utils.py:441] 2022-05-16 13:38:28,287 >> Configuration saved in /home/komleva/gen_money/out/config.json\n",
            "[INFO|modeling_utils.py:1378] 2022-05-16 13:38:30,368 >> Model weights saved in /home/komleva/gen_money/out/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-05-16 13:38:30,369 >> tokenizer config file saved in /home/komleva/gen_money/out/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-05-16 13:38:30,369 >> Special tokens file saved in /home/komleva/gen_money/out/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     1.3191\n",
            "  train_runtime            = 0:47:32.22\n",
            "  train_samples            =       1506\n",
            "  train_samples_per_second =       2.64\n",
            "  train_steps_per_second   =       2.64\n",
            "05/16/2022 13:38:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2416] 2022-05-16 13:38:30,457 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-05-16 13:38:30,457 >>   Num examples = 171\n",
            "[INFO|trainer.py:2421] 2022-05-16 13:38:30,457 >>   Batch size = 8\n",
            "100%|███████████████████████████████████████████| 22/22 [00:14<00:00,  1.70it/s]05/16/2022 13:38:46 - INFO - datasets.metric - Removing /home/komleva/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "100%|███████████████████████████████████████████| 22/22 [00:15<00:00,  1.44it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.7545\n",
            "  eval_loss               =     1.3048\n",
            "  eval_runtime            = 0:00:16.16\n",
            "  eval_samples            =        171\n",
            "  eval_samples_per_second =     10.575\n",
            "  eval_steps_per_second   =      1.361\n",
            "  perplexity              =     3.6868\n",
            "[WARNING|training_args.py:999] 2022-05-16 13:38:46,629 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:999] 2022-05-16 13:38:46,630 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|modelcard.py:460] 2022-05-16 13:38:47,522 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7545345932442706}]}\n"
          ]
        }
      ],
      "metadata": {
        "id": "yyV_rTL3ZE_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e1bccb-a8bf-4616-dcc8-d6eb893cf7d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using the model**\n",
        "Next lets take our model we just trained and use it to generate some text! We will import the Tensorflow version of the gpt2 language model and set the from_pt flag to True. Then we load a pretrained tokenizer from huggingface. This may take some time to download the tokenizer data."
      ],
      "metadata": {
        "id": "3CpzI5mU1jPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "#! python -m pip install https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-2.3.0-cp37-cp37m-win_amd64.whl\n",
        "! python -m pip install -i https://pypi.org/project/tensorflow/#:~:text=tensorflow%2D2.8.0%2Dcp37%2Dcp37m%2Dmanylinux2010_x86_64.whl"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "! pip install torch"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from torch) (4.1.1)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "%cd /home/komleva/gen_money"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/komleva/gen_money\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# setup imports to use the model\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"out\", from_pt=True)\n",
        "#model = TFGPT2Model.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-05-16 13:39:49.762601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2022-05-16 13:39:49.762629: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-05-16 13:39:49.762902: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-05-16 13:39:49.806915: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'lm_head.weight', 'transformer.h.20.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.4.attn.masked_bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "metadata": {
        "id": "kFOx9AUa1tnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding sample text is now extremely simple using the pretrained tokenizer."
      ],
      "metadata": {
        "id": "3oWWo4HJ4wd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "input_ids = tokenizer.encode(\"<|title|>1 dollar :\", return_tensors='tf')"
      ],
      "outputs": [],
      "metadata": {
        "id": "6xT0tc07_-SL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# the tf tensor object\n",
        "input_ids[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([  27,   91, 7839,   91,   29,   16, 8872, 1058], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "lxtUSIAc_-1G",
        "outputId": "23eb4da1-04d8-4711-b716-980831ef8c39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will use the model to generate the text from our input sample. The parameters I used are based on trail and error from playing around with the huggingface tutorial, https://huggingface.co/blog/how-to-generate, which really goes into great detail on how to go about finding the best parameters for generating text. As well they dive into really good information on what each parameter does and how they play into one another."
      ],
      "metadata": {
        "id": "BZ3YNTNlBsh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "generated_text_samples = model.generate(\n",
        "    input_ids, \n",
        "    max_length=150,  \n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ],
      "metadata": {
        "id": "NbzHNvvaAPns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "#Print output for each sequence generated above\n",
        "for i, beam in enumerate(generated_text_samples):\n",
        "  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: <|title|>1 dollar : It is equal to 1$ (US.$1 = 111.45).\n",
            "1: <|title|>1 dollar : The conversion rate in the public finance ministry's books was set at 100 tugrik (or one U.S.$) = 1 Ethiopian birr, which would translate into roughly $2 to 0 foreign currency equivalent per day for most people - less than a few dollars for those working on rooftops and motorbikes\n",
            "2: <|title|>1 dollar : For example, one U.S./Canada 1-dollar bill contains 12 cents and costs $2 in the United States (though coins are not required).\n",
            "3: <|title|>1 dollar : The cost of the one-way ticket, which costs 300 rubles (one U.S.$0 bill), will be 589 kopecks ($572).\n",
            "4: <|title|>1 dollar : Additionally, commencing on the first day of the month following full execution hereof a new one-dollar banknote shall be issued in denominations no less than five dollars and twenty cents, each with an upper limit not to exceed one hundred fifty thousand dollars.\n"
          ]
        }
      ],
      "metadata": {
        "id": "XPdteSR_B3w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "And there you have it, a simple end to end outline on how you can use Colab, Huggingface, and Tensorflow to train and generate new text data using GPT-2. There is a lot of playing around with hyperparameters in the generate phase but given enough tweaking and time you can usually find something that works well with your data and task. I found that even with the larger GPT-2 model and more examples, it could still repeat itself a bit so something you have to generate a large number of sequences before you get a set that you like. Even OpenAI made note of this in their initial results for GPT-2 so if at first it doesnt generate what you want keep trying and playing with the parameters!\n",
        "\n",
        "One tip I did notice was that if you do not setup your examples with a start token, then you run into the issue of repeated phrases more easily. Given more data that might be less of a problem but I ran into that a lot before putting in the start token of <|title|> in my exmaples. This start token also has the added benefit of giving you a generic starting point in the text generation so that each run is mostly unique from the last run if you do not care about having a specific prompt."
      ],
      "metadata": {
        "id": "GekD8zzvCq3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "! pip install openai"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (0.18.1)\n",
            "Requirement already satisfied: requests>=2.20 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from openai) (1.2.0.58)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: tqdm in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: et-xmlfile in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pandas>=1.2.3->openai) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from pandas-stubs>=1.1.0.11->openai) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.20->openai) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/komleva/anaconda3/envs/GPUEnv/lib/python3.7/site-packages (from requests>=2.20->openai) (3.3)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-002\",\n",
        "  prompt=\"\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=256,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_139333/1964151066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n",
            "\u001b[0;32m~/anaconda3/envs/GPUEnv/lib/python3.7/site-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/GPUEnv/lib/python3.7/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mapi_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mapi_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/GPUEnv/lib/python3.7/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m     81\u001b[0m     ):\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_base\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         self.api_type = (\n\u001b[1;32m     85\u001b[0m             \u001b[0mApiType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/GPUEnv/lib/python3.7/site-packages/openai/util.py\u001b[0m in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         raise openai.error.AuthenticationError(\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;34m\"No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
          ]
        }
      ],
      "metadata": {
        "id": "OLZEvT-ACz1R"
      }
    }
  ]
}